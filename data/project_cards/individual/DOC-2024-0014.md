# Automating Multilingual Census Data Processing: An AI and Transformer-Based Pipeline for Efficient Language Detection and Translation for Short-Text
## DOC-2024-0014
_[Department of Commerce](../3_agency/Department of Commerce.md)_ (DOC) / CENSUS - U.S. Census Bureau


+ **The topic area that most closely aligns with the AI use case**: Mission-Enabling (internal agency support)
+ **Whether the use case is implemented solely with the commercial-off-the-shelf or freely available AI products identified in Appendix B**: None of the above.
+ **Description of the AI’s intended purpose, including: (1) what problem the AI is used to solve and (2) the expected benefits and positive outcomes from the AI for an agency’s mission and/or the general public**: The purpose of this AI-driven pipeline is to tackle the challenge of processing non-English, short-text responses in large-scale surveys and censuses, especially brief entries for race and ethnicity. Traditional language detection and translation systems often struggle with these minimal text responses, impacting data accuracy and inclusivity. This AI solution is designed to automate the entire multilingual data processing workflow, from language detection to translation, named entity recognition, and validation, using AI and transformer-based models along with natural language processing techniques. By achieving high accuracy even with limited context, the system reduces the need for human translators, increases processing speed, and guarantees fair and accurate representation of diverse populations. This innovation not only supports the agency’s mission to collect inclusive, representative data but also benefits the public by contributing to more precise demographic insights, ultimately aiding in resource allocation and policy making.
+ **Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc**: The AI system outputs a series of automated decisions and validated translations for short-text responses, such as race and ethnicity write-ins, within large-scale survey data. Specifically, it provides accurate language detection, corrects potential spelling errors, generates contextually accurate translations, and validates these translations against standardized labels using semantic similarity analysis. The system then selects the most accurate translation for each input, guaranteeing precise categorization and extraction of demographic data. These outputs streamline survey data processing by reducing the need for human intervention, allowing for real-time or near real-time responses that meet high standards of accuracy and inclusivity
+ **The current stage of System Development Life Cycle (SDLC) for the AI use case (See System Development Life Cycle (SDLC) - Glossary | NIST CSRC)**: Acquisition and/or Development
+ **Whether the AI use case is rights-impacting or safety-impacting, as defined in Section 6 of OMB Memorandum M-24-10. Note also that Appendix I of that memorandum lists categories of use cases that are presumed by default to impact rights and safety. If your use case falls into those lists, you must not answer “neither” to this question without a formal determination made by your agency’s Chief AI Officer, pursuant to Section 5(b) of OMB Memorandum M-24-10**: Neither
+ **Date when the AI use case’s purpose and high-level requirements were first defined**: 6/3/2024
+ **Date when the acquisition and/or development of the AI system associated with the use case first began**: 7/8/2024
+ **Date when the AI use case was fully implemented and deployed into use**: 9/30/2025
+ **Whether the AI system involved in the use case was developed, or is expected to be developed, exclusively with contracting resources, in-house, or a combination of both**: Developed in-house.
+ **Whether the AI use case supports a High-Impact Service Provider (HISP) public-facing service. The full list of HISP public-facing services can be found at https://www.performance.gov/cx/hisps/**: No
+ **General description of the data used for training, fine-tuning, and evaluation of the AI use case’s model(s). This should include a description of any data provided by the agency, whether the datasets are research datasets, publicly available, external, etc. and to the extent possible, sufficiently descriptive information from the vendor**: The agency used agency-owned decennial paradata to train, fine-tune, and evaluate the AI model’s performance. This paradata, collected from previous census responses, provided important interaction data that allowed the model to improve its language detection and translation accuracy, especially for short-text responses typical in race and ethnicity write-ins.
+ **Whether the data has sufficient documentation of its integrity, quality, and validity as a training set for a specific task**: Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.
+ **Identification of the demographic features explicitly utilized in the AI use case’s data/model(s), if any**: Race/Ethnicity
+ **Whether this AI project includes custom-developed code**: Yes
+ **Whether the agency has access to the code associated with the AI use case**: Yes – agency has access to source code, but it is not public.
+ **Whether the AI use case itself has an associated Authority to Operate (ATO), or is part of a system that has an ATO**: No
+ **Estimated length of time the AI project waited for basic computing and developer tools to build machine learning models. This includes Government Furnished Equipment (GFE), along with access to open-source code libraries, frameworks, and other coding software or developer environments**: Less than 6 months
+ **Whether the required IT infrastructure is identified and provisioned via a centralized process (such as a unified intake form) that enables product owners and developers to find the right development environment inside the agency**: Yes
+ **Explanation for whether the AI use case has an identifiable process to request computing resources (such as cloud computing credits or computing hardware) to develop and test models**: Yes
+ **Has the communication been timely**: Yes
+ **Whether the AI use case promotes re-use of existing infrastructure to promote AI development**: None: This use case does not re-use any internally developed tooling or managed infrastructure from any other AI development efforts within the agency.
+ **Whether information regarding the AI use case has been made widely accessible within the agency**: Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.