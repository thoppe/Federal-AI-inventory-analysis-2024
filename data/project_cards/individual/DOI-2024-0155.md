# Data-driven approaches to filling missing time-series data within the San Francisco Bay-Delta
## DOI-2024-0155
_[Department of the Interior](<../3_agency/Department of the Interior.md>)_ (DOI) / USGS


+ **The topic area that most closely aligns with the AI use case**: Mission-Enabling
+ **Whether the use case is implemented solely with the commercial-off-the-shelf or freely available AI products identified in Appendix B**: None of the above.
+ **Description of the AI’s intended purpose, including: (1) what problem the AI is used to solve and (2) the expected benefits and positive outcomes from the AI for an agency’s mission and/or the general public**: In measurements of natural systems, incomplete time-series data are the rule, not the exception. Environmental time-series data may suffer from gaps at a variety of time scales, significantly reducing the number of observations to understand phenomena, identify change, calibrate models, and predict future behavior. Data may be missing because of sensor degradation (e.g., biofouling, mechanical issues, power failure), failure to meet basic quality assurance checks, or resampling of paired observations to a common timestamp.

Decades of water-quality time-series data (e.g., turbidity, salinity, temperature) collected throughout the San Francisco Bay-Delta (SFBD) by a variety of agencies including USGS, CA DWR, USBR, USFWS, and USACE are used as a critical indicator of estuary health. These data are no exception to the rule and contain numerous gaps. While bad data points can be relatively straightforward to identify (e.g., as anomalously high or low values), they are challenging to replace and are often flagged and left blank, which can bias long-term observational records.

We will test and develop several (~5) proposed methods to fill gaps in water-quality time-series data and quantify the uncertainty in those estimates. The methods will range in complexity, from linear regression to machine learning and deep learning approaches (Lepot and others, 2017), and will characterize uncertainty in the filled data (Cox and others, 2003).  For this proposal, we will initially restrict our scope to turbidity data, because of the relatively high biofouling rate of the optical sensors typically used to measure turbidity, as well as its ecological significance. In addition, we are already actively implementing one of the methods to fill gaps in turbidity data collected in a south S.F. Bay salt-marsh tidal creek (Figure 1) as part of a project investigating temporal variability in sediment delivery to marshes funded by the S.F. Bay Regional Monitoring Program.
+ **Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc**: Outputs were not documented and DOI will update the use cases once additional data is collected from submitter
+ **The current stage of System Development Life Cycle (SDLC) for the AI use case (See System Development Life Cycle (SDLC) - Glossary | NIST CSRC)**: Acquisition and/or Development
+ **Whether the AI use case is rights-impacting or safety-impacting, as defined in Section 6 of OMB Memorandum M-24-10. Note also that Appendix I of that memorandum lists categories of use cases that are presumed by default to impact rights and safety. If your use case falls into those lists, you must not answer “neither” to this question without a formal determination made by your agency’s Chief AI Officer, pursuant to Section 5(b) of OMB Memorandum M-24-10**: Neither
+ **Whether the AI system involved in the use case was developed, or is expected to be developed, exclusively with contracting resources, in-house, or a combination of both**: Developed in-house.
+ **Whether the AI use case supports a High-Impact Service Provider (HISP) public-facing service. The full list of HISP public-facing services can be found at https://www.performance.gov/cx/hisps/**: No
+ **Whether the AI use case disseminates information to the public**: Yes
+ **Description of how the agency is ensuring that this AI use case is compliant with Information Quality Act guidelines and OMB Memorandum M-19-15, if applicable**: USGS Fundamental Science Practicies Policies
+ **Whether the AI use case involves personally identifiable information (PII), as defined in OMB Circular A-130**: No
+ **Whether the agency’s Senior Agency Official for Privacy (SAOP) has assessed the privacy risks associated with this AI use case**: No
+ **Whether the AI use case made use of agency-wide infrastructure to quickly identify the right datasets to begin model development**: Yes
+ **Whether the data has sufficient documentation of its integrity, quality, and validity as a training set for a specific task**: Documentation is complete: Documentation exists regarding the maintenance, composition, quality, and intended use of the training and evaluation data, as well as any statistical bias across model features and protected groups.
+ **Identification of the demographic features explicitly utilized in the AI use case’s data/model(s), if any**: None
+ **Whether this AI project includes custom-developed code**: Yes
+ **Whether the agency has access to the code associated with the AI use case**: Yes – agency has access to source code, but it is not public.
+ **Whether the AI use case itself has an associated Authority to Operate (ATO), or is part of a system that has an ATO**: Yes
+ **Name of the system(s) associated with this AI use case, according to the ATO**: Science Support System (SSS)
+ **Estimated length of time the AI project waited for basic computing and developer tools to build machine learning models. This includes Government Furnished Equipment (GFE), along with access to open-source code libraries, frameworks, and other coding software or developer environments**: Less than 6 months
+ **Whether the required IT infrastructure is identified and provisioned via a centralized process (such as a unified intake form) that enables product owners and developers to find the right development environment inside the agency**: Yes
+ **Explanation for whether the AI use case has an identifiable process to request computing resources (such as cloud computing credits or computing hardware) to develop and test models**: Yes
+ **Has the communication been timely**: Yes
+ **Whether the AI use case promotes re-use of existing infrastructure to promote AI development**: Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.
+ **Whether information regarding the AI use case has been made widely accessible within the agency**: Documentation has been published: Complete documentation has been published to a repository or data catalog within the agency and is made accessible to other data science teams for review and feedback.
+ **Whether the agency requested an extension for this AI use case, in response to the CAIO’s request, of up to one year for the minimum requirements outlined in Section 5 of OMB Memorandum M-24-10**: No – Agency did not request an extension for this use case.