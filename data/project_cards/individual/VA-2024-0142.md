# Recovery Engagement and Coordination for Health – Veterans Enhanced Treatment (REACH VET) 1.0 predictive model
## VA-2024-0142
_[Department of Veterans Affairs](../3_agency/Department of Veterans Affairs.md)_ (VA) / VHA: Veterans Health Administration


+ **The topic area that most closely aligns with the AI use case**: Health & Medical
+ **Whether the use case is implemented solely with the commercial-off-the-shelf or freely available AI products identified in Appendix B**: None of the above.
+ **Description of the AI’s intended purpose, including: (1) what problem the AI is used to solve and (2) the expected benefits and positive outcomes from the AI for an agency’s mission and/or the general public**: The REACH VET 1.0 model is a predictive model using predictors extracted from the patient’s medical records (into the VA’s Corporate Data Warehouse), such as diagnoses, prescriptions, and health care utilization, to estimate statistical risk of a suicide death in the next month.  
This predictive model is used for two purposes. (1) Primary use case is to support identification of patients at elevated statistical risk of suicide in the next month for referral to a clinical case review and outreach program. Staffing guidance to ensure each health care system is staffed to conduct the case review and outreach protocol is established per clinical policy. To date, the population recommended to the clinical case review program has been limited to a number equivalent to 0.1% of the total treated patient population at each facility each month; the number of patients in the program may be modified based on guidance from the Office of Suicide Prevention. The REACH VET 1.0 statistical risk estimates contribute to an algorithm used to identify patients to refer to this program at each facility. (2) Secondary use case is to help risk stratify patients based on prior health care interactions to support augmentation of clinical attention and interventions for patients with patterns of care associated with high suicide risk. This is used, for example, to support clinical review of relevant health care history during triage of Veteran Crisis Line calls and during risk assessment of patients in mental health care.
For use case 1, REACH VET coordinators and REACH VET providers use the list and supporting information to guide execution of the targeted prevention program. For use case 2, Veterans Crisis Line responders and VHA health care providers use the categorized risk scores in the context of curated descriptive data from the patient’s medical record to review risk factors towards optimizing assessment and clinical follow-up.   
The REACH VET 1.0 model is expected to identify a population of patients at elevated risk of suicide and other negative outcomes for review by clinicians; clinicians conduct telephone outreach to identified patients when they deem such outreach appropriate.  This model and targeted prevention in which it is used (i.e. the REACH VET program) is used to augment VHA's extensive clinical suicide prevention program. While VHA conducts universal screening for suicide risk by asking patients structured questions regarding suicidality, this screening process does not identify all patients who go on to die of  suicide. The REACH VET model is intended to identify patients who might be missed in clinical assessment or whose risk may have changed since their last assessment. Here, the REACH VET model identifies patients at statistical risk of suicide, that is they share similarities in their health care data to patients who did die of suicide in the past.  The patients at highest statistical risk are highlighted to mental health providers for additional review. This targeted prevention program has been nationally implemented in VHA health care systems since 2017 and an evaluation of the effects of the REACHVET program was conducted. This evaluation found that REACH VET implementation was associated with greater treatment engagement and new safety plan documentation and fewer mental health admissions, emergency department visits, and suicide attempts. Full findings of this program evaluation are available at:  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8524305/
+ **Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc**: The REACH VET predictive model estimates the likelihood of dying of suicide in the next month for each active patient in VHA. This estimate is used as part of an algorithm that considers factors such as the size of the health care system, current patient treatment engagement, and whether the patient has had previous engagement with the REACH VET program to generate a dashboard with patients at high statistical risk of suicide for review by a REACH VET coordinator and identified clinicians at each health care system. The REACH VET dashboard provides information about identified risk factors for the included patients and supports a coordinated case review and, where applicable, clinical outreach process to support identification of care gaps and offer additional services as appropriate.
+ **The current stage of System Development Life Cycle (SDLC) for the AI use case (See System Development Life Cycle (SDLC) - Glossary | NIST CSRC)**: Operation and Maintenance
+ **Whether the AI use case is rights-impacting or safety-impacting, as defined in Section 6 of OMB Memorandum M-24-10. Note also that Appendix I of that memorandum lists categories of use cases that are presumed by default to impact rights and safety. If your use case falls into those lists, you must not answer “neither” to this question without a formal determination made by your agency’s Chief AI Officer, pursuant to Section 5(b) of OMB Memorandum M-24-10**: Both
+ **Date when the AI use case’s purpose and high-level requirements were first defined**: 1/1/2014
+ **Date when the acquisition and/or development of the AI system associated with the use case first began**: 1/1/2014
+ **Date when the AI use case was fully implemented and deployed into use**: 11/1/2016
+ **Whether the AI system involved in the use case was developed, or is expected to be developed, exclusively with contracting resources, in-house, or a combination of both**: Developed in-house.
+ **Whether the AI use case supports a High-Impact Service Provider (HISP) public-facing service. The full list of HISP public-facing services can be found at https://www.performance.gov/cx/hisps/**: No
+ **Whether the AI use case disseminates information to the public**: No
+ **Whether the AI use case involves personally identifiable information (PII), as defined in OMB Circular A-130**: Yes
+ **Whether the agency’s Senior Agency Official for Privacy (SAOP) has assessed the privacy risks associated with this AI use case**: Yes
+ **Whether the AI use case made use of agency-wide infrastructure to quickly identify the right datasets to begin model development**: Yes
+ **General description of the data used for training, fine-tuning, and evaluation of the AI use case’s model(s). This should include a description of any data provided by the agency, whether the datasets are research datasets, publicly available, external, etc. and to the extent possible, sufficiently descriptive information from the vendor**: As quoted in publication 'Clinical data came from the VHA National Patient Care Database, an archival data set that contains comprehensive patient-by-patient, encounter-by-encounter clinical and administrative data derived from the Department of Veterans Affairs’ electronic medical records. Data regarding vital status and cause of death were from the National Death Index, a centralized database maintained by the National Center for Health Statistics of death record information on file in state vital statistics offices. We conducted searches with established procedures.24 We constructed 3 samples: (1) model development, (2) model validation, and (3) prediction. The development and validation samples contained data for patient-months from all patients who died from suicide from October 1, 2008, to September 30, 2011, and had received VHA services in the previous 2 years (case patients) and a random 1% of patients who survived the month and received VHA services in the previous 2 years (control patients). We randomly assigned half of the case patients and half of the control patients to the development sample and half to the validation sample. Each contained patient-months from 3180 case patients and 1 056 004 control patients.' (McCarthy et al, 2015).  Data from the VA Corporate Data Warehouse was used to evaluate performance of the models.
+ **Whether the data has sufficient documentation of its integrity, quality, and validity as a training set for a specific task**: Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.
+ **Identification of the demographic features explicitly utilized in the AI use case’s data/model(s), if any**: ["Age","Sex/Gender","Marital Status","Race/Ethnicity","Ability Status"]
+ **Identification of the other demographic features explicitly utilized in the AI use case’s data/model(s), if any**: Note that "Ability status" is service connected disability status in this model.
+ **Whether this AI project includes custom-developed code**: Yes
+ **Whether the agency has access to the code associated with the AI use case**: Yes – agency has access to source code, but it is not public.
+ **Whether the AI use case itself has an associated Authority to Operate (ATO), or is part of a system that has an ATO**: Yes
+ **Name of the system(s) associated with this AI use case, according to the ATO**: Corporate Data Warehouse - 1152
+ **Estimated length of time the AI project waited for basic computing and developer tools to build machine learning models. This includes Government Furnished Equipment (GFE), along with access to open-source code libraries, frameworks, and other coding software or developer environments**: Less than 6 months
+ **Whether the required IT infrastructure is identified and provisioned via a centralized process (such as a unified intake form) that enables product owners and developers to find the right development environment inside the agency**: Yes
+ **Explanation for whether the AI use case has an identifiable process to request computing resources (such as cloud computing credits or computing hardware) to develop and test models**: Yes
+ **Has the communication been timely**: Yes
+ **Whether the AI use case promotes re-use of existing infrastructure to promote AI development**: Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.
+ **Whether information regarding the AI use case has been made widely accessible within the agency**: Documentation has been developed: Complete documentation detailing model performance across a range of benchmarks, architecture, relevant features and information regarding the appropriate use of the model for predictive tasks has been created.
+ **Whether the agency requested an extension for this AI use case, in response to the CAIO’s request, of up to one year for the minimum requirements outlined in Section 5 of OMB Memorandum M-24-10**: No – Agency did not request an extension for this use case.
+ **Whether an AI impact assessment for the AI use case has been completed, consistent with Section 5(c)(iv)(A) of OMB Memorandum M-24-10. This assessment should document at least the intended purpose for the AI and its expected benefits, the potential risks of using the AI, and the quality and appropriateness of the relevant data**: Yes
+ **Whether the AI use case has been tested for performance in a real-world environment to understand its potential impact on individual(s) or communities, consistent with Section 5(c)(iv)(B) of OMB Memorandum M-24-10**: Impact evaluation in operational environment: The AI use case has been tested in an operational environment before being fully implemented as a solution and has utilized randomized experiments with a control group or other counterfactual, social systems analysis, or other rigorous research methodologies to evaluate impact and identify potential harm to users as well as broader groups of people.
+ **Identification of the reasonably foreseeable risks from using the AI use case, to include the risk of inequitable outcomes for individuals. This can be informed by publicly known risks identified in similar use cases or contexts. Examples of such risks include at least physical injury, emotional/psychological injury, opportunity loss, economic loss, privacy loss, environmental impact, or civil rights and civil liberties impact**: Our main concerns regarding risks are as follows:  (1) Providers might over-rely on statistical risk estimate. To reduce this risk, we provide information regarding which risk factors were identified by the predictive model to encourage transparent understanding of the basis for the model estimate.  Moreover, the clinical prevention program provides only general guidelines to clinicians indicating that identified patients should be reviewed.  This forces clinicians to rely on their clinical skills to plan intervention responses. (2) The program could be clinically inefficient and use provider time that might be more effectively focused elsewhere.  This is a real risk which may evolve over time as other components of the overall VA suicide prevention program improve, and potentially reduce the need for this targeted prevention program. VHA conducts ongoing evaluations to assess for potential inefficiencies in the model and test methods to improve clinical efficiency.  For example, VA is currently piloting a process to expand focus of the REACH VET program to patients not currently engaged in mental health care.  (3) The program could lead to unfair allocation of clinical resources if the model is biased and other clinical programming does not compensate for areas of model underperformance. This is also a risk and thus VHA evaluates model bias across key demographic populations and actively develops methods to reduce model bias or address bias by adjusting other components of the treatment system to compensate.   After review of the common risks template, the following additional risks were identified: Fair & Equitable (FE) - Algorithm target not reflective of real-world outcome of interest Response: There is a minor risk that the county coroners who assign cause of death might have miscategorized some Veteran deaths, reducing the validity of the outcome variable used.   Transparent & Explainable (TE) - Degradation of End-User Trust and Ineffective Challenge or Remedy Processes Response: This is a risk. There has been public misunderstanding of how the model performs and is used. VA has responded and clarified misconceptions in the public forums in which misunderstanding has arisen.   Accountable & Monitored (AM) - Performance Efficacy and Fairness (e.g. Model/Data Drift, Model Degradation, or inappropriate application)  Response: This is a risk, model performance is degrading over time and an update is in process. However all identified patients are clinically complex and appropriate for clinical attention.  Accountable & Monitored (AM) - User-Introduced Errors Response: There is a low risk, there is a community of practice to support proper implementation of the REACH VET Program, which is supervised and trained by national program leads.   Accountable & Monitored (AM) - System Performance Not as Intended Response: There is a low risk, the internal VA team that manages the model and risk estimates based on it has validation processes in place to review all steps of the process.
+ **Whether an independent evaluation of the AI use case has been conducted through the CAIO, an agency AI oversight board, or other appropriate agency office with existing test and evaluation responsibilities, consistent with Section 5(c)(iv)(C) of OMB Memorandum M-24-10. The independent reviewing authority must not have been directly involved in the AI system’s development**: TRUE
+ **Whether there’s an established process for monitoring performance of the AI system’s functionality and changes to its impact on rights and/or safety, including, if relevant, the risk of AI-enabled discrimination, consistent with Section 5(c)(iv)(D) of OMB Memorandum M-24-10**: Intermittent and Manually Updated: A plan for monitoring the AI use case is in place, and requires data science teams to work with DevOps engineers to manually update models at scheduled intervals, and create metrics to detect data distribution shifts between the operational environment and the training data for the model.
+ **Whether the agency has determined that the AI can carry out its individual core decisions or actions that could result in a significant impact on rights and safety without direct human involvement. The answer should not be “yes” if humans are not reviewing individual decisions and are only overseeing or reviewing the performance of the system in general**: No - Some individual decisions or actions require direct human oversight.
+ **How the agency is providing reasonable and timely notice regarding the use of AI, consistent with Section 5(c)(iv)(I) of OMB Memorandum M-24-10**: Telephone
+ **How the agency has assessed whether there are significant disparities in the AI’s performance across demographic groups, including in the AI’s real-world deployment, consistent with Section 5(c)(v)(A) of OMB Memorandum M-24-10**: Yes
+ **How the agency has consulted affected groups, including underserved communities, in the design, development, and use of the AI, consistent with Section 5(c)(v)(B) of OMB Memorandum M-24-10**: VA has examined suicide risk concentration among Veterans in VHA care, stratified by patient sex, race and ethnicity.  Model performance was reasonably similar across demographic subgroups. Because the REACH VET clinical program is just one augmentative component of the overall suicide prevention program, weaknesses in the ability of the REACH VET algorithm to identify high risk patients for intervention have been considered in design of other components of the overall suicide prevention program and mitigated where possible.  For example, the REACH VET model lacks information about patients who have had minimal engagement with the health care system for suicide risk factors they may have experienced.  As such, the model will inherently underestimate risk for new patients and patients experiencing new episodes of symptoms or stressors that could elevate risk.  To combat this weakness, VA implemented a universal suicide risk screening process, including at least annual screening of all patients in primary care and mandatory suicide screening in VA emergency departments which are likely to be the entry point for patients with new challenges that are not yet documented in medical records. This screening triggers mandatory follow-up for positive cases by a clinician with mental health care training, who can offer services (as would be done in REACH VET triggered outreach).  As another example, some known risk factors for suicide were not well captured in medical records at the time of model training.  VHA has implemented clinical screening programs for some of these risk factors (e.g. military sexual trauma, food insecurity) and trained clinicians regarding their contribution to risk and the importance of offering services to address challenges.  Likewise, VA decision support systems that provide drill-in summaries of suicide related risks for REACH VET and other VHA patients (i.e. CRISTAL) summarize and display information from these clinical screeners with other information about suicide risk factors to facilitate clinical evaluation and response to suicide risks.  While it is impossible to build a REACH VET model that is not affected by biases in detection of risk factors due to variation in access to/utilization of VA health care, VHA has taken steps to address expected/known gaps in model performance through implementation of other suicide prevention programming.
+ **How the agency has consulted affected groups, including underserved communities, in the design, development, and use of the AI, consistent with Section 5(c)(v)(B) of OMB Memorandum M-24-10**: ["Post-transaction customer feedback collections","Other","Public hearings or meetings"]
+ **Whether the agency has established a fallback and escalation process, which may include reverting to manual human processes, in the event that an impacted individual would like to appeal or contest an AI system’s outcome, consistent with Section 5(c)(v)(E) of OMB Memorandum M-24-10**: Yes
+ **Whether the agency has established an opt-out mechanism, consistent with Section 5(c)(v)(F) of OMB Memorandum M-24-10**: Yes