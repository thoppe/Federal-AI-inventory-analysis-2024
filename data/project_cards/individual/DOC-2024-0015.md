# FAQ for SMaRT
## DOC-2024-0015
_Department of Commerce_ (DOC) / CENSUS - U.S. Census Bureau


+ **The topic area that most closely aligns with the AI use case**: Government Services (includes Benefits and Service Delivery)
+ **Whether the use case is implemented solely with the commercial-off-the-shelf or freely available AI products identified in Appendix B**: Searching for information using AI.
+ **Description of the AI’s intended purpose, including: (1) what problem the AI is used to solve and (2) the expected benefits and positive outcomes from the AI for an agency’s mission and/or the general public**: The purpose of the FAQ search tool is to find the correct Census Bureau-provided answers to common questions from the public about the 2025 Census Test. The 2025 Census Test is a proposed test as part of the Small-Scale Response Test (SmaRT) program in decennial. The purpose of this program is to perform experiments using different methods to improve self-response rates and data collection. The SmaRT program accomplishes this using iterative tests with relatively small sample sizes. Four installments of the SmaRT program were fielded before the 2020 Census. Three small-scale tests have occurred so far this decade with the fourth test planned for 2025. There is an online questionnaire for each of the tests, with a census.gov landing page to access the online questionnaire. In the first three tests this decade, there were approximately 15 FAQs on the landing page that the user could read. For the 2025 Census Test, we would like to offer over 50 FAQs to answer the public’s questions via a search tool.
Current public-facing search tools at the Census Bureau use exact match only. Depending on what users type to search, the correct FAQ might not appear because users do not always use the same terminology as the Census Bureau. This FAQ search tool built with machine-learning models will use semantic similarity (i.e., synonyms or misspellings) to find the correct answers. The answers it provides back to the public are census-approved answers, and not generative AI answers. If the 2025 Census Test is approved, this search feature will be on the landing page for the survey. Implementing this search tool is considered a proof-of-concept for the use of such a search feature. If it is successful, the search tool could be used in the 2026 Census Test on its landing page, or the 2028 Dress Rehearsal. Ultimately, if it is successful, the public will benefit by finding answers to common questions more easily and more accurately.
+ **Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc**: Description of what the AI system outputs, whether it’s a prediction, recommendation, decision, etc.
Utilizing transfer learning from Large Language Models (LLMs) we utilize them to encode our Census approved FAQs to then compare to a user query. This user query is then encoded as an embedding from the same LLM model and then we take a distance calculation using cosine similarity to rank our reference materials. We then showcase the top 3 results depending if a threshold is met of .25 cosine similarity or above. It is a semantic search tool that ranks reference material.
+ **The current stage of System Development Life Cycle (SDLC) for the AI use case (See System Development Life Cycle (SDLC) - Glossary | NIST CSRC)**: Acquisition and/or Development
+ **Whether the AI use case is rights-impacting or safety-impacting, as defined in Section 6 of OMB Memorandum M-24-10. Note also that Appendix I of that memorandum lists categories of use cases that are presumed by default to impact rights and safety. If your use case falls into those lists, you must not answer “neither” to this question without a formal determination made by your agency’s Chief AI Officer, pursuant to Section 5(b) of OMB Memorandum M-24-10**: Neither
+ **Date when the AI use case’s purpose and high-level requirements were first defined**: 7/8/2024
+ **Date when the acquisition and/or development of the AI system associated with the use case first began**: 9/9/2024
+ **Date when the AI use case was fully implemented and deployed into use**: 7/11/2025
+ **Whether the AI system involved in the use case was developed, or is expected to be developed, exclusively with contracting resources, in-house, or a combination of both**: Developed in-house.
+ **Whether the AI use case supports a High-Impact Service Provider (HISP) public-facing service. The full list of HISP public-facing services can be found at https://www.performance.gov/cx/hisps/**: No
+ **General description of the data used for training, fine-tuning, and evaluation of the AI use case’s model(s). This should include a description of any data provided by the agency, whether the datasets are research datasets, publicly available, external, etc. and to the extent possible, sufficiently descriptive information from the vendor**: We did not fine tune the LLM because it was just used for transfer learning. We created a question-and-answer pairing dataset to estimate search accuracy. We also selected FAQs that had a high use rate in the 2020 Census to populate the 2025 FAQ list.
+ **Whether the data has sufficient documentation of its integrity, quality, and validity as a training set for a specific task**: Documentation has been partially completed: Some documentation exists (detailing the composition and any statistical bias or measurement skew for training and evaluation purposes), but documentation took place within this use case’s development.
+ **Whether this AI project includes custom-developed code**: Yes
+ **Whether the agency has access to the code associated with the AI use case**: Yes – agency has access to source code, but it is not public.
+ **Whether the AI use case itself has an associated Authority to Operate (ATO), or is part of a system that has an ATO**: No
+ **Estimated length of time the AI project waited for basic computing and developer tools to build machine learning models. This includes Government Furnished Equipment (GFE), along with access to open-source code libraries, frameworks, and other coding software or developer environments**: Less than 6 months
+ **Whether the required IT infrastructure is identified and provisioned via a centralized process (such as a unified intake form) that enables product owners and developers to find the right development environment inside the agency**: Yes
+ **Explanation for whether the AI use case has an identifiable process to request computing resources (such as cloud computing credits or computing hardware) to develop and test models**: Yes
+ **Has the communication been timely**: Yes
+ **Whether the AI use case promotes re-use of existing infrastructure to promote AI development**: Use of existing data platforms: This use case is being developed on existing enterprise data and analytics platforms within the agency rather than procuring additional platforms or SaaS to operate.
+ **Whether information regarding the AI use case has been made widely accessible within the agency**: Limited documentation for review: Some information regarding the model’s performance on specific benchmarks are available or have been shared with specific stakeholders within the agency.